diff --git a/lie_detector/datasets/trial_dataset.py b/lie_detector/datasets/trial_dataset.py
index 1223e99..5b38a43 100644
--- a/lie_detector/datasets/trial_dataset.py
+++ b/lie_detector/datasets/trial_dataset.py
@@ -33,7 +33,7 @@ ANNOTATION_CSV_FILENAME = 'TrialData/Annotation/All_Gestures_Deceptive and Truth
 class TrialDataset(Dataset):
 
 
-    def __init__(self, subsample_fraction: float = None, num_folds: int = 3, frames_per_sample = 64):
+    def __init__(self, subsample_fraction: float = None, num_folds: int = 3, frames = 64):
         if not os.path.exists(str(PROCESSED_DATA_FILENAME)):
             _download_and_process_trial()
 
@@ -46,7 +46,7 @@ class TrialDataset(Dataset):
         self.trn_folds = []
         self.val_folds = []
         self.input_shape = [224, 224, 3]
-        self.frames_per_sample = frames_per_sample
+        self.frames_per_sample = frames
 
 
     def load_or_generate_data(self):
diff --git a/lie_detector/models/base.py b/lie_detector/models/base.py
index 7c8e38e..71a9e64 100644
--- a/lie_detector/models/base.py
+++ b/lie_detector/models/base.py
@@ -7,6 +7,8 @@ from keras.callbacks import EarlyStopping
 from time import time
 from typing import Dict, Optional
 from sklearn.metrics import mean_squared_error
+from wandb.keras import WandbCallback
+import wandb
 
 from lie_detector.datasets.dataset_sequence import DatasetSequence
 from lie_detector.datasets.dataset import Dataset
@@ -85,14 +87,11 @@ class Model:
             early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3, verbose=1, mode='auto')
             callbacks.append(early_stopping)
 
-        # Hide lines below until Lab 4
         if use_wandb:
-            wandb.init()
+            # wandb.init(project='fs-lie-detector', group='kfolds')
             wandb_callback = WandbCallback()
             callbacks.append(wandb_callback)
 
-        # model.network.summary()
-
         t = time()
         history = self.fit(dataset=dataset, batch_size=batch_size, epochs=epochs, callbacks=callbacks)
         print('Training took {:2f} s'.format(time() - t))
@@ -114,8 +113,8 @@ class Model:
         return 'binary_crossentropy'
 
     def optimizer(self):
-        if 'optimizer' in self.network_args:
-            return self.network_args['optimizer']
+        if 'learning_rate' in self.network_args:
+            return Adam(lr=self.network_args['learning_rate'])
         return Adam()
 
     def metrics(self):
diff --git a/lie_detector/networks/lstm_network.py b/lie_detector/networks/lstm_network.py
index f712bbb..f1c4c35 100644
--- a/lie_detector/networks/lstm_network.py
+++ b/lie_detector/networks/lstm_network.py
@@ -20,7 +20,7 @@ from keras import layers
 
 # if face_net_model is given, input of shape (batches, frames, x, y, 1)
 # if face_net_model is none, input of shape (batches, frames, features)
-def LSTM(frames=64, face_net_model=None, hidden_units=64, weights=None, input_shape=None, dropout=0.5):
+def LSTM(frames=64, face_net_model=None, end2end="False", lstm_units=64, dense_units=32, weights=None, input_shape=None, dropout=0.5, learning_rate=None):
 
     img_input = Input(shape=[frames,]+input_shape)
 
@@ -30,11 +30,11 @@ def LSTM(frames=64, face_net_model=None, hidden_units=64, weights=None, input_sh
         bn_axis = 1
 
     x = img_input
-    if face_net_model is not None:
+    if end2end is not "False" and face_net_model is not None:
     	x = TimeDistributed(face_net_model, input_shape=(frames, face_net_model.output_shape[1]))(x)
 
-    x = layers.LSTM(units=hidden_units, return_sequences=False, dropout=dropout)(x)
-    x = Dense(32, activation='relu')(x)
+    x = layers.LSTM(units=lstm_units, return_sequences=False, dropout=dropout)(x)
+    x = Dense(dense_units, activation='relu')(x)
     x = Dropout(dropout)(x)
 
     x = Dense(1, activation='sigmoid')(x)
diff --git a/lie_detector/training/experiments/sample.json b/lie_detector/training/experiments/sample.json
index 52923db..5eec05d 100644
--- a/lie_detector/training/experiments/sample.json
+++ b/lie_detector/training/experiments/sample.json
@@ -3,13 +3,38 @@
     "experiments": [
         {
             "dataset": "TrialDataset",
-            "model": "End2EndModel",
-            "network": "lstm",
+            "model": "BaseModel",
+            "feature_model": "CNNModel",
+            "base_network": "LSTM",
+            "head_network": "RESNET50",
             "network_args": {
-                "num_layers": 2
+                "lstm_units": 64,
+                "dense_units": 32,
+                "frames": 64, 
+                "end2end": "False",
+                "learning_rate": 0.0001
             },
             "train_args": {
-                "batch_size": 8
+                "batch_size": 8,
+                "epochs": 10
+            }
+        },
+        {
+            "dataset": "TrialDataset",
+            "model": "BaseModel",
+            "feature_model": "CNNModel",
+            "base_network": "LSTM",
+            "head_network": "RESNET50",
+            "network_args": {
+                "lstm_units": 32,
+                "dense_units": 32,
+                "frames": 64, 
+                "end2end": "False",
+                "learning_rate": 0.0001
+            },
+            "train_args": {
+                "batch_size": 8,
+                "epochs": 10
             }
         }
     ]
diff --git a/lie_detector/training/prepare_experiments.py b/lie_detector/training/prepare_experiments.py
index 68ccb3e..e147a22 100644
--- a/lie_detector/training/prepare_experiments.py
+++ b/lie_detector/training/prepare_experiments.py
@@ -13,7 +13,7 @@ def run_experiments(experiments_filename):
     for ind in range(num_experiments):
         experiment_config = experiments_config['experiments'][ind]
         experiment_config['experiment_group'] = experiments_config['experiment_group']
-        print("pipenv run python training/run_experiment.py --gpu=0 {}".format(json.dumps(experiment_config)))
+        print("python lie_detector/training/run_experiment.py --wandb --gpu=0 --experiment_config \'{}\'".format(json.dumps(experiment_config)))
 
 
 def main():
diff --git a/lie_detector/training/run_experiment.py b/lie_detector/training/run_experiment.py
index b6e2969..178aa37 100644
--- a/lie_detector/training/run_experiment.py
+++ b/lie_detector/training/run_experiment.py
@@ -77,7 +77,7 @@ def run_experiment(experiment_config: Dict, save_weights: bool, gpu_ind: int, us
     experiment_config['experiment_group'] = experiment_config.get('experiment_group', None)
     experiment_config['gpu_ind'] = gpu_ind
     
-    if experiment_config['end2end'] == "False":
+    if experiment_config['network_args']['end2end'] == "False":
         print('Initializing feature model...')
         feature_model_class_ = getattr(models_module, experiment_config['feature_model'])
         feature_model = feature_model_class_(network_fn=head_network_fn_)
@@ -97,7 +97,7 @@ def run_experiment(experiment_config: Dict, save_weights: bool, gpu_ind: int, us
         )
 
         if use_wandb:
-            wandb.init()
+            wandb.init(project='fs-lie-detector', group='kfolds')
             wandb.config.update(experiment_config)
         
         print('Beginning training...')
@@ -145,9 +145,9 @@ def _parse_args():
         default="{\"dataset\": \"TrialDataset\", \"model\": \"LSTMModel\", \"network\": \"LSTM\", \"end2end\": \"False\"}"
     )
     parser.add_argument(
-        "--nowandb",
-        default=True,
-        action='store_false',
+        "--wandb",
+        default=False,
+        action='store_true',
         help='If true, do not use wandb for this run'
     )
     args = parser.parse_args()
@@ -159,7 +159,7 @@ def main():
     args = _parse_args()
     experiment_config = json.loads(args.experiment_config)
     os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu)
-    run_experiment(experiment_config, args.save, args.gpu, use_wandb=not args.nowandb)
+    run_experiment(experiment_config, args.save, args.gpu, use_wandb=args.wandb)
 
 
 if __name__ == '__main__':
diff --git a/tasks/train_lstm.sh b/tasks/train_lstm.sh
index 7df6f30..a964d9c 100644
--- a/tasks/train_lstm.sh
+++ b/tasks/train_lstm.sh
@@ -1,2 +1,2 @@
 #!/bin/bash
-python lie_detector/training/run_experiment.py --save --experiment_config '{"dataset": "TrialDataset", "feature_model":"CNNModel", "model": "BaseModel", "base_network": "LSTM", "head_network": "RESNET50", "train_args": {"batch_size": 8}, "network_args": {"frames":64}, "end2end": "False"}'
+python lie_detector/training/run_experiment.py --save --wandb --experiment_config '{"dataset": "TrialDataset", "feature_model":"CNNModel", "model": "BaseModel", "base_network": "LSTM", "head_network": "RESNET50", "train_args": {"batch_size": 8}, "network_args": {"frames":64}, "end2end": "False"}'
diff --git a/web/client/src/App.js b/web/client/src/App.js
index 8a756c8..e4eed7c 100644
--- a/web/client/src/App.js
+++ b/web/client/src/App.js
@@ -64,7 +64,9 @@ class App extends Component {
     }
     else {
       this.setState({show_file_upload_progress: true,
-                     show_spinner: true})
+                     show_spinner: true,
+                     stage: 'none',
+                     completed_stages: []})
       const data = new FormData() 
       data.append('file', this.state.selectedFile)
 
diff --git a/web/client/src/components/StageSummary.js b/web/client/src/components/StageSummary.js
index 6a815e4..1699fd1 100644
--- a/web/client/src/components/StageSummary.js
+++ b/web/client/src/components/StageSummary.js
@@ -5,7 +5,7 @@ export default class StageSummary extends React.Component {
 
     render() {
         return (<div className="form-group col-md-5 center-div mt-3">
-                  <h2>Stage {this.props.id + 1} Complete</h2>
+                  <h3>Stage {this.props.id + 1} Complete</h3>
                   <p>{this.props.stage} finished successfully</p>
                 </div>)
     }
